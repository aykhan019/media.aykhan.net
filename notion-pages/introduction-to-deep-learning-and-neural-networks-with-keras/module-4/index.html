<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Module 4: Deep Learning Models</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}
/* For Edge (based on WebKit) */
::-webkit-scrollbar {
    width: 7px;
}

::-webkit-scrollbar-track {
    background: transparent; /* Transparent background for the track */
}

::-webkit-scrollbar-thumb {
    background: rgba(0, 0, 0, 0.5); /* Semi-transparent thumb */
    border-radius: 10px; /* Rounded corners for the thumb */
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI Variable Display", "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-default_background {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-default_background {
	color: inherit;
	fill: inherit;
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 237, 214, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(0, 0, 0, 0.06); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(249, 228, 188, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="ccafb3f1-ff20-42b1-8975-1c787448de2d" class="page sans"><header><img class="page-cover-image" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/Introduction_to_Deep_Learning__Neural_Networks_with_Keras.png" style="object-position:center 15.969999999999995%"/><div class="page-header-icon page-header-icon-with-cover"><img class="icon" src="https://www.notion.so/icons/science_purple.svg"/></div><h1 class="page-title">Module 4: Deep Learning Models</h1><p class="page-description"></p></header><div class="page-body"><h1 id="1f4bfc83-c1cf-4133-889f-215bc8e859cd" class="">Deep Neural Networks: An Introduction</h1><h2 id="f79915a3-945d-42e1-a6ce-82824072bb25" class="">Overview</h2><p id="e697a30a-81f3-45d7-a4c0-f8c3a350ce70" class="">Neural networks have evolved significantly over time, transitioning from shallow networks to deep neural networks (DNNs) that handle complex tasks and data types. This note explores the distinctions between shallow and deep neural networks, and the reasons behind the recent advancements in deep learning.</p><h2 id="a0d960a5-b489-4343-b295-c252610df1c4" class="">Shallow vs. Deep Neural Networks</h2><h3 id="bff45a90-c977-4745-a018-84167c232fe1" class="">Shallow Neural Networks</h3><ul id="f0328beb-1c20-4c7a-b08e-7de901fa3803" class="bulleted-list"><li style="list-style-type:disc"><strong>Definition</strong>: A shallow neural network typically has only one hidden layer.</li></ul><ul id="a02c97f4-06c7-4461-bd31-2809c399f403" class="bulleted-list"><li style="list-style-type:disc"><strong>Characteristics</strong>:<ul id="15bfcbbc-d220-492b-8d83-dd0228369a1c" class="bulleted-list"><li style="list-style-type:circle">Simpler architectures</li></ul><ul id="2869bf7d-d09b-4180-a7ff-378251ea4204" class="bulleted-list"><li style="list-style-type:circle">Limited ability to extract features from raw data</li></ul><ul id="99e15f20-8004-46a6-99fa-d3a9144b8c78" class="bulleted-list"><li style="list-style-type:circle">Often used for simpler tasks or as building blocks for deeper networks</li></ul></li></ul><h3 id="c9dba60a-5c13-4e2a-a1a3-39ec3db360ef" class="">Deep Neural Networks</h3><ul id="a8368cc8-ebd8-41ec-9c3e-3a10062d168c" class="bulleted-list"><li style="list-style-type:disc"><strong>Definition</strong>: A deep neural network has multiple hidden layers and neurons.</li></ul><ul id="ebfbc6f1-97cd-4c80-9f6e-607400367aa4" class="bulleted-list"><li style="list-style-type:disc"><strong>Characteristics</strong>:<ul id="8f65d06b-1bf9-4599-842b-e4bba04455d7" class="bulleted-list"><li style="list-style-type:circle">Capable of learning hierarchical features from raw data such as images and text</li></ul><ul id="24bf3266-899d-4e72-8817-13660ecc3a2b" class="bulleted-list"><li style="list-style-type:circle">Ability to extract features from raw data.</li></ul><ul id="86f2ac71-a425-4322-9be6-6ce067bd2dbd" class="bulleted-list"><li style="list-style-type:circle">Handles more complex tasks and datasets</li></ul><ul id="adfe02ac-31f1-4618-a524-e4c16004ee75" class="bulleted-list"><li style="list-style-type:circle">Example: Image recognition, natural language processing</li></ul></li></ul><h2 id="95a72c35-3303-4cd3-b9cb-1d4398bed5e6" class="">Factors Contributing to the Rise of Deep Learning</h2><h3 id="31bcb8b5-e8b3-4d3e-a135-5c0ac24a762b" class="">1. Advancements in Neural Network Techniques</h3><ul id="5e0ca91d-093c-4a84-9287-4f5e88d7cd0e" class="bulleted-list"><li style="list-style-type:disc"><strong>ReLU Activation Function</strong>:<ul id="c4d8dc03-750e-4fc7-b8bb-f5d57979344d" class="bulleted-list"><li style="list-style-type:circle">Addressed the vanishing gradient problem</li></ul><ul id="e7b1928f-a7ea-43d7-a93f-903f4bb6e42b" class="bulleted-list"><li style="list-style-type:circle">Enabled the training of very deep networks</li></ul></li></ul><h3 id="3099f4a6-d5f6-4397-852d-431b25ff59b4" class="">2. Availability of Data</h3><ul id="f7b386c4-2f2d-4a2c-b589-870373f647bd" class="bulleted-list"><li style="list-style-type:disc"><strong>Importance</strong>:<ul id="a613f2ec-86d6-4a73-9fd3-26d8d04ab218" class="bulleted-list"><li style="list-style-type:circle">Deep neural networks excel with large datasets</li></ul><ul id="6cb8b182-2379-4b94-a85a-1bafe547c2f5" class="bulleted-list"><li style="list-style-type:circle">Large data helps prevent overfitting</li></ul></li></ul><ul id="bf29aa80-911e-4a2b-8950-2e948896c761" class="bulleted-list"><li style="list-style-type:disc"><strong>Current Trends</strong>:<ul id="be45e23a-b6f1-4907-9b1f-6c7c38f032a4" class="bulleted-list"><li style="list-style-type:circle">Access to vast amounts of data has become easier</li></ul><ul id="9d328bc2-1f3e-4c1d-8098-eb5e8d1332a3" class="bulleted-list"><li style="list-style-type:circle">Deep learning algorithms benefit significantly from increased data</li></ul></li></ul><h3 id="d9653357-2f01-43ba-85da-ba4214647242" class="">3. Computational Power</h3><ul id="eea9c658-60ff-4235-9382-e03b6c13852f" class="bulleted-list"><li style="list-style-type:disc"><strong>GPU Utilization</strong>:<ul id="210a526e-83f4-419d-929d-0856538756db" class="bulleted-list"><li style="list-style-type:circle">NVIDIA GPUs and other high-performance computing resources</li></ul><ul id="f1b72d7c-15e2-42da-bb58-583204fbb7fb" class="bulleted-list"><li style="list-style-type:circle">Reduced training times from weeks to hours</li></ul></li></ul><ul id="320441fc-57e1-46e3-bc0b-30e8cbcc2ec3" class="bulleted-list"><li style="list-style-type:disc"><strong>Impact</strong>:<ul id="d2a3fa6b-7e6d-4643-bc64-bc4d7b15a8c8" class="bulleted-list"><li style="list-style-type:circle">Enables experimentation with different network architectures and prototypes in shorter periods</li></ul></li></ul><h2 id="10da6717-2cab-486d-8d2b-840fbdf5dbbe" class="">Conclusion</h2><p id="28503cf8-14f8-407e-8a3c-45e99ec8ffd6" class="">The combination of advancements in neural network techniques, the availability of large datasets, and increased computational power has driven the recent boom in deep learning. The field continues to evolve, with deep neural networks becoming increasingly prevalent in various applications. Upcoming topics will delve into supervised deep learning algorithms and convolutional neural networks (CNNs).</p><hr id="9873dfaf-dec8-4ba8-bb67-f46d4f9b2ed9"/><h1 id="f2ab13ea-fdb0-41f2-afb9-aa95d313599f" class="">Introduction to Convolutional Neural Networks (CNNs) (Supervised Deep Learning Model)</h1><h2 id="8f2694f0-8797-45d6-8dbf-c372ed496e99" class="">Overview</h2><p id="d8f215b7-2ff7-48e6-87b0-7e7c047ed175" class="">Convolutional Neural Networks (CNNs) are a specialized type of neural network designed for processing structured grid data such as images. This note covers the fundamental architecture of CNNs, their operational mechanisms, and how to build them using the Keras library.</p><h3 id="7a180dbb-ce66-4211-bbfe-bcead3110f29" class="">Convolutional Neural Networks (CNNs)</h3><h2 id="11dcf26e-9c08-4a6a-b126-bc2c67295c20" class="">Architecture</h2><figure id="15d453af-626f-47a7-a27b-f9f07af27eca" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image.png"><img style="width:708px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image.png"/></a></figure><h3 id="ad21efd6-92b9-472f-82c2-aa9dbc54e7da" class="">Image Input Dimensions</h3><ul id="20861462-5a25-428d-bc4c-487218a04b95" class="bulleted-list"><li style="list-style-type:disc"><strong>Grayscale Images</strong>: (n x m x 1)</li></ul><ul id="ab5e1ff6-002f-493b-bdb5-677a5321a543" class="bulleted-list"><li style="list-style-type:disc"><strong>Colored Images</strong>: (n x m x 3), where 3 represents RGB channels.</li></ul><figure id="a6cdbb39-bcbd-4364-b80e-fab07aa5cdde" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%201.png"><img style="width:708px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%201.png"/></a></figure><h2 id="a3c1663a-fd97-4842-ac7a-fc5db041ff22" class="">Key Components of CNNs</h2><h3 id="bc603b14-4986-4db3-bc23-a92b08b84835" class="">1. Convolutional Layer</h3><ul id="9be83c27-ec69-4ff8-8b17-f128f97d63af" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: Applies filters to the input image to produce feature maps.</li></ul><ul id="3604da2b-fa48-457e-a853-833b816f513b" class="bulleted-list"><li style="list-style-type:disc"><strong>Filter</strong>: A small matrix used to detect features such as edges, textures, or patterns.</li></ul><ul id="fff1e9d0-2598-487c-b084-0c017fb2d4dd" class="bulleted-list"><li style="list-style-type:disc"><strong>Operation</strong>: Computes the convolution operation between the filter and the input image.<ul id="f8e767e3-73fb-40cc-8d41-2dfce57bfb5c" class="bulleted-list"><li style="list-style-type:circle"><strong>Filter Size</strong>: e.g., (2 x 2)</li></ul><ul id="43497713-3755-4018-b294-8a6dca07b739" class="bulleted-list"><li style="list-style-type:circle"><strong>Stride</strong>: The number of pixels by which the filter moves across the image.</li></ul><ul id="40f91d9f-1477-480d-888e-f8e0fcbaeb37" class="bulleted-list"><li style="list-style-type:circle"><strong>Output</strong>: An empty matrix filled with results from the convolution process</li></ul></li></ul><figure id="24dd498b-998f-4817-9ce5-1c533eb42e76" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%202.png"><img style="width:708px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%202.png"/></a></figure><h3 id="6a807b04-38f0-4f09-9b42-dcc92dd88dd0" class="">2. Activation Function (ReLU)</h3><ul id="cca1218f-bb0d-41e8-a69a-4b59256f0353" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: Introduces non-linearity into the model.</li></ul><ul id="f5e101a4-a382-485b-b670-cff214220d55" class="bulleted-list"><li style="list-style-type:disc"><strong>Operation</strong>: Applies the ReLU (Rectified Linear Unit) function to the output of the convolutional layer.<ul id="f4fe13b0-042a-4d22-8506-bec39299eafc" class="bulleted-list"><li style="list-style-type:circle"><strong>ReLU Function</strong>: Outputs the input directly if it is positive; otherwise, it outputs zero.</li></ul></li></ul><figure id="e0d53053-a60c-469c-a2d2-094ff4e966f5" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%203.png"><img style="width:1560px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%203.png"/></a></figure><h3 id="46c67353-8637-4744-bba3-da7167a673a3" class="">3. Pooling Layer</h3><ul id="fe8ed324-94ee-477f-bd16-c80e18d8e737" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: Reduces the spatial dimensions of the feature maps.</li></ul><ul id="416beb3b-a922-42ef-8e53-f2053f36d858" class="bulleted-list"><li style="list-style-type:disc"><strong>Types</strong>:<ul id="febd957e-e4c9-41cd-b3cd-fbc70844c8c3" class="bulleted-list"><li style="list-style-type:circle"><strong>Max-Pooling</strong>: Selects the maximum value from each section of the feature map.<ul id="b4f3e209-6908-462a-a8ee-85699c1cbe75" class="bulleted-list"><li style="list-style-type:square"><strong>Filter Size</strong>: e.g., (2 x 2)</li></ul><ul id="4bfac16d-98c5-41ac-b7b2-17b63521798a" class="bulleted-list"><li style="list-style-type:square"><strong>Stride</strong>: The number of pixels by which the pooling filter moves.</li></ul></li></ul><ul id="3c18f3bc-4950-4d7e-9591-bed5a168bac8" class="bulleted-list"><li style="list-style-type:circle"><strong>Average-Pooling</strong>: Computes the average value from each section of the feature map.</li></ul></li></ul><ul id="db16044c-cb10-4b11-a6e1-24cf12d9a02e" class="bulleted-list"><li style="list-style-type:disc"><strong>Benefit</strong>: Reduces computational complexity and helps prevent overfitting.</li></ul><figure id="19b4830f-7abe-42e2-960a-b562e99ca8fa" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%204.png"><img style="width:708px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%204.png"/></a></figure><figure id="64955086-295f-44f1-9a5f-96fd40fbec7d" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%205.png"><img style="width:708px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%205.png"/></a></figure><h3 id="e90d263a-2050-4efc-8bbc-891ef7750177" class="">4. Fully Connected Layer</h3><ul id="d95fc606-9b8f-41fa-8091-614e93fefad2" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: Connects every node from the previous layer to every node in the current layer.</li></ul><ul id="40d4108f-6ae4-4e85-a274-e063cefb0983" class="bulleted-list"><li style="list-style-type:disc"><strong>Operation</strong>: Flattens the output from the previous layers and produces an n-dimensional vector, where n corresponds to the number of classes.</li></ul><ul id="554cd004-5c6b-45e8-9c18-61390b04a00f" class="bulleted-list"><li style="list-style-type:disc"><strong>Activation Function</strong>: Typically uses the softmax function to convert the outputs into probabilities.</li></ul><figure id="4284a012-5a8d-411e-9fc7-6931da123b8b" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%206.png"><img style="width:708px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%206.png"/></a></figure><h2 id="dae2a4aa-6626-46d9-95bd-e70f5b31bc4c" class="">CNN Architecture</h2><ul id="1626d356-2bc3-46a7-bf52-0437d6418fb5" class="bulleted-list"><li style="list-style-type:disc"><strong>Input Layer</strong>: Defines the size of the input images (e.g., 128 x 128 x 3 for color images).</li></ul><ul id="c0c9e71d-7549-4592-a5bd-07c74b410e1f" class="bulleted-list"><li style="list-style-type:disc"><strong>Convolutional Layers</strong>: Apply multiple filters and include ReLU activation.</li></ul><ul id="4c4733a0-98d7-4ecd-8e0e-1643a850e6ae" class="bulleted-list"><li style="list-style-type:disc"><strong>Pooling Layers</strong>: Apply max-pooling or average-pooling.</li></ul><ul id="6bef9eae-5482-4d2f-a3d7-fc6be488bf50" class="bulleted-list"><li style="list-style-type:disc"><strong>Fully Connected Layers</strong>: Flatten the data and produce output based on the number of classes.</li></ul><ul id="8a05ffb4-b99d-45f8-8238-d9d97f8c6768" class="bulleted-list"><li style="list-style-type:disc"><strong>Output Layer</strong>: Produces the final class probabilities using the softmax activation function.</li></ul><h2 id="cf5639f9-5cb5-4753-8cb3-3dc02d528711" class="">Summary</h2><ul id="8da7dc06-740f-43ef-bae3-08e38430034f" class="bulleted-list"><li style="list-style-type:disc"><strong>Efficiency</strong>: CNNs reduce the number of parameters compared to traditional neural networks, making them computationally efficient.</li></ul><ul id="a051e6f3-7cf6-44a9-a158-b41ee8f97484" class="bulleted-list"><li style="list-style-type:disc"><strong>Applications</strong>: Ideal for tasks involving image recognition, object detection, and other computer vision problems.</li></ul><h2 id="137fa129-177d-4002-994e-e26b09450549" class="">Building CNNs with Keras</h2><ol type="1" id="3cb03ee0-e36b-443c-9eaa-10134dcfa6ca" class="numbered-list" start="1"><li><strong>Model Creation</strong>:<ul id="a3294aad-cd0f-4e33-80c9-9ac562020a9b" class="bulleted-list"><li style="list-style-type:disc">Use the <code>Sequential</code> model to build the CNN.</li></ul></li></ol><ol type="1" id="8071dba7-9d8f-4ce3-b479-5503f570ba14" class="numbered-list" start="2"><li><strong>Defining Input Shape</strong>:<ul id="fff34575-a838-800a-b9ce-f5be416983a9" class="bulleted-list"><li style="list-style-type:disc">For 128x128 color images: <code>input_shape=(128, 128, 3)</code></li></ul></li></ol><ol type="1" id="9e79e61b-3936-4800-a69a-c830b088e5e0" class="numbered-list" start="3"><li><strong>Adding Layers</strong>:<ul id="0bbaf502-6987-4548-b91f-69f22e9408eb" class="bulleted-list"><li style="list-style-type:disc"><strong>Convolutional Layer</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="a01319fc-5d26-42ac-8625-8c1610a15d68" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">model.add(Conv2D(16, (2, 2), strides=(1, 1), activation=&#x27;relu&#x27;, input_shape=(128, 128, 3)))</code></pre><ul id="f2c28ef8-1e19-483f-b686-6bfe84affe8c" class="bulleted-list"><li style="list-style-type:circle"><strong>Parameters</strong>:<ul id="ad1adb01-e315-433a-953e-aefbe5e7f00c" class="bulleted-list"><li style="list-style-type:square"><code>16</code>: Number of filters or kernels.</li></ul><ul id="b7bf0c4a-390b-4e4a-aace-0700e960f243" class="bulleted-list"><li style="list-style-type:square"><code>(2, 2)</code>: Size of each filter.</li></ul><ul id="84e205de-6c1d-4221-a783-aeaf816b98ed" class="bulleted-list"><li style="list-style-type:square"><code>strides=(1, 1)</code>: The step size with which the filter moves across the image.</li></ul><ul id="6307d4c9-9514-4c11-a0c6-7f99b63b4899" class="bulleted-list"><li style="list-style-type:square"><code>activation=&#x27;relu&#x27;</code>: Activation function applied after the convolution operation.</li></ul><ul id="6566b4ed-f1e6-4653-bfec-a522aeaec950" class="bulleted-list"><li style="list-style-type:square"><code>input_shape=(128, 128, 3)</code>: Shape of the input images. For color images, the last dimension is 3 (RGB channels).</li></ul></li></ul></li></ul><ul id="11e0b512-baf1-4be1-b27e-a8f31d0ba166" class="bulleted-list"><li style="list-style-type:disc"><strong>Pooling Layer</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="ddb569dc-bf80-43d7-8476-8c00d5aa358b" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))</code></pre><ul id="8c3169a2-a052-49be-bb35-2a68a85abb68" class="bulleted-list"><li style="list-style-type:circle"><strong>Parameters</strong>:<ul id="477af083-79a2-4e27-ba0f-73f250ac10e2" class="bulleted-list"><li style="list-style-type:square"><code>pool_size=(2, 2)</code>: Size of the pooling window.</li></ul><ul id="71e8ac61-2b0e-4a60-9750-31a11add6a11" class="bulleted-list"><li style="list-style-type:square"><code>strides=(2, 2)</code>: The step size with which the pooling window moves across the image.</li></ul></li></ul></li></ul><ul id="c1d6e17d-7b56-43ef-91be-c109e3a45c6a" class="bulleted-list"><li style="list-style-type:disc"><strong>Additional Convolutional and Pooling Layers</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="301b1cad-f79e-4c5f-99c5-6eea0c8281a8" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">model.add(Conv2D(32, (2, 2), strides=(1, 1), activation=&#x27;relu&#x27;))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))</code></pre><ul id="6760850b-e9d1-4cab-9b46-236c15a0bfbc" class="bulleted-list"><li style="list-style-type:circle"><strong>Parameters</strong>:<ul id="a903b913-27e4-448a-a1ee-f5a2d2a7abec" class="bulleted-list"><li style="list-style-type:square"><code>32</code>: Number of filters or kernels in this convolutional layer.</li></ul><ul id="34cfd76f-8795-49a3-8bb1-3c535b8f6420" class="bulleted-list"><li style="list-style-type:square"><code>(2, 2)</code>: Size of each filter.</li></ul><ul id="000c49a2-39b8-40d9-8f3d-c24cd7f763bd" class="bulleted-list"><li style="list-style-type:square"><code>strides=(1, 1)</code>: The step size with which the filter moves across the image.</li></ul><ul id="7fd591b5-fd5f-4a7b-8409-0fa8bdbcea17" class="bulleted-list"><li style="list-style-type:square"><code>activation=&#x27;relu&#x27;</code>: Activation function applied after the convolution operation.</li></ul><ul id="82d641a1-2e74-432c-825d-824a762013d3" class="bulleted-list"><li style="list-style-type:square"><code>pool_size=(2, 2)</code>: Size of the pooling window.</li></ul><ul id="f9ed1d2b-54c5-4169-98ec-ed82918ca3e8" class="bulleted-list"><li style="list-style-type:square"><code>strides=(2, 2)</code>: The step size with which the pooling window moves across the image.</li></ul></li></ul></li></ul></li></ol><ol type="1" id="bd4869b9-6452-4f34-9b68-783ba2b9091a" class="numbered-list" start="4"><li><strong>Flattening and Fully Connected Layers</strong>:<ul id="7c151cf6-3a56-4afc-a2fa-48df2ea580bd" class="bulleted-list"><li style="list-style-type:disc"><strong>Flatten</strong>: Converts 3D feature maps into 1D.<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="d54ef089-0a9f-402f-a103-8fa1a6303a46" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">model.add(Flatten())</code></pre></li></ul><ul id="fff34575-a838-8057-92a2-c678b84539c6" class="bulleted-list"><li style="list-style-type:disc"><strong>Dense Layers</strong>:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="8af5a7f7-bd2c-4a36-9ac9-40081360ff2c" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">model.add(Dense(100, activation=&#x27;relu&#x27;))
model.add(Dense(num_classes, activation=&#x27;softmax&#x27;))</code></pre><ul id="f0cb151d-752b-4134-aaa8-af2f49cf0b37" class="bulleted-list"><li style="list-style-type:circle"><strong>Parameters</strong>:<ul id="8babd05e-2914-4eed-bbce-87c22d726c17" class="bulleted-list"><li style="list-style-type:square"><code>100</code>: Number of neurons in the dense layer.</li></ul><ul id="4060330f-1d6e-453b-b1f7-ac699fd51fb8" class="bulleted-list"><li style="list-style-type:square"><code>activation=&#x27;relu&#x27;</code>: Activation function applied to the neurons in the dense layer.</li></ul><ul id="f29c3e48-4de3-437a-ad4b-7013817bebfb" class="bulleted-list"><li style="list-style-type:square"><code>num_classes</code>: Number of output neurons, typically equal to the number of classes in the classification problem.</li></ul><ul id="6d350923-c9c1-4e32-9a4b-abc05b1b5f37" class="bulleted-list"><li style="list-style-type:square"><code>activation=&#x27;softmax&#x27;</code>: Activation function applied to the output layer to convert raw scores into probabilities.</li></ul></li></ul></li></ul></li></ol><ol type="1" id="519a1edf-de38-4be0-8e19-40e7649ba2ec" class="numbered-list" start="5"><li><strong>Compilation</strong>:<ul id="2e4fc00b-f4f1-4384-b152-ac2e566394c4" class="bulleted-list"><li style="list-style-type:disc">Define optimizer, loss function, and metrics:<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/><pre id="f92e7f85-b4fb-4ffc-884b-d08795158e07" class="code"><code class="language-Python" style="white-space:pre-wrap;word-break:break-all">model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])</code></pre><ul id="378d103f-342c-41e9-ba84-c903322d88ba" class="bulleted-list"><li style="list-style-type:circle"><strong>Parameters</strong>:<ul id="e8d5ab0e-005f-4b04-a95c-06f0c3bcb33b" class="bulleted-list"><li style="list-style-type:square"><code>optimizer=&#x27;adam&#x27;</code>: Optimization algorithm used for training.</li></ul><ul id="2fb88c33-fa0a-4254-be40-c737a5765ea0" class="bulleted-list"><li style="list-style-type:square"><code>loss=&#x27;categorical_crossentropy&#x27;</code>: Loss function used for classification tasks with multiple classes.</li></ul><ul id="ad30d725-f4f5-4b02-9244-edbcda1dbf6a" class="bulleted-list"><li style="list-style-type:square"><code>metrics=[&#x27;accuracy&#x27;]</code>: Evaluation metric used to measure the performance of the model.</li></ul></li></ul></li></ul></li></ol><ol type="1" id="9c0c5212-d2e7-4ab1-9461-c9f5dec08bbc" class="numbered-list" start="6"><li><strong>Training and Validation</strong>:<ul id="d2a4cf4b-19df-47c0-abe3-d82aa487b0bc" class="bulleted-list"><li style="list-style-type:disc">Train the model using the <code>fit</code> method and validate using a test set.</li></ul></li></ol><figure id="26311204-1034-499b-9dc9-f74946dcdc1c" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%207.png"><img style="width:708px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%207.png"/></a></figure><h2 id="9a665f33-87c4-4219-b180-e9ec0143f8f9" class="">Conclusion</h2><p id="04554441-2e6f-4014-bf01-75f36a58ba71" class="">CNNs are powerful for image processing tasks due to their ability to automatically extract and learn features from images. The architecture involves convolutional, activation, pooling, and fully connected layers, which collectively enable the network to perform tasks such as image recognition and object detection.</p><hr id="19a2d852-55a0-4ae1-9571-2d7f86b7551a"/><h1 id="8009d064-4419-4ea2-807d-fa762ec6939c" class="">Recurrent Neural Networks (RNNs) Overview (Unsupervised Deep Learning Model)</h1><h2 id="40ac12de-20a0-415d-a7f2-3baab9d1bfb2" class="">Introduction to RNNs</h2><ul id="719d8239-ad05-4fe8-b99e-357e0caf3fbb" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: RNNs are used to analyze sequences where data points are not independent but rather follow a temporal or sequential relationship.</li></ul><ul id="b5df4788-81f0-4e42-8178-d49ca01b7ee9" class="bulleted-list"><li style="list-style-type:disc"><strong>Architecture</strong>: RNNs include loops that allow them to take both the current input and the output from the previous data point into account.</li></ul><h2 id="1ec488c5-a643-427b-b47f-f7c690043761" class="">How RNNs Work</h2><ul id="9a48a9f0-1cfe-447d-8902-144c8ac12497" class="bulleted-list"><li style="list-style-type:disc"><strong>Input and Output at Each Time Step</strong>:<ul id="dff6a6fa-2d23-4129-bdb1-fce02bf1f2a2" class="bulleted-list"><li style="list-style-type:circle">At time <code>t = 0</code>, the network takes in input <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span> and outputs <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">a_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>.</li></ul><ul id="2d17fa02-b641-483d-8342-896a08bf0ebd" class="bulleted-list"><li style="list-style-type:circle">At time <code>t = 1</code>, the network takes in input <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span><code> </code>and the previous output <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">a_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>, weighted by <style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css')</style><span data-token-index="0" contenteditable="false" class="notion-text-equation-token" style="user-select:all;-webkit-user-select:all;-moz-user-select:all"><span></span><span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mn>1</mn><mo separator="true">,</mo><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">w_{1,2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></span><span>﻿</span></span>.</li></ul><ul id="f5a23303-d223-4fec-97bf-1866b0df7b62" class="bulleted-list"><li style="list-style-type:circle">This process continues, incorporating previous outputs into the computation at each step.</li></ul></li></ul><figure id="9d282c19-617f-4bb5-91de-5d4accbedcc6" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%208.png"><img style="width:1561px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%208.png"/></a></figure><h2 id="80cf251b-8e69-4c0d-983c-776138a07497" class="">Applications of RNNs</h2><ul id="2da5a320-300e-4282-a695-397b0854d60f" class="bulleted-list"><li style="list-style-type:disc"><strong>Text Analysis</strong>: Suitable for processing and modeling sequential text data.</li></ul><ul id="219a4629-fba3-4955-a546-9d541407bd08" class="bulleted-list"><li style="list-style-type:disc"><strong>Genomic Data</strong>: Can analyze sequences in genetic information.</li></ul><ul id="3be61f18-eac6-4e92-9ca1-ee721aa5c534" class="bulleted-list"><li style="list-style-type:disc"><strong>Handwriting</strong>: Applied in handwriting generation and recognition.</li></ul><ul id="e8f5a004-5dbb-48c6-9749-38a83c06dbd7" class="bulleted-list"><li style="list-style-type:disc"><strong>Stock Markets</strong>: Used for predicting stock prices based on historical data.</li></ul><h2 id="88c7eab3-a7e5-4007-8d45-144c0504a393" class="">Long Short-Term Memory (LSTM) Networks</h2><ul id="d0e1b591-7d4c-4918-a641-a5e8ee77f69b" class="bulleted-list"><li style="list-style-type:disc"><strong>Overview</strong>: A specialized type of RNN designed to overcome some limitations of traditional RNNs.</li></ul><ul id="4371e419-d57c-44a2-a08a-ee78d7474100" class="bulleted-list"><li style="list-style-type:disc"><strong>Applications</strong>:<ul id="9fa4f044-e7e5-4090-b010-7ebe40e5d8cb" class="bulleted-list"><li style="list-style-type:circle"><strong>Image Generation</strong>: Models trained on images to generate novel images.</li></ul><ul id="5ce6e565-82ee-47d2-94c1-cef315ae5df6" class="bulleted-list"><li style="list-style-type:circle"><strong>Handwriting Generation</strong>: Creating handwritten text based on trained models.</li></ul><ul id="3f3733da-e58f-4a97-9e97-b7d717753c9f" class="bulleted-list"><li style="list-style-type:circle"><strong>Image Description</strong>: Automatically describing images.</li></ul><ul id="d5f9029c-cf85-4435-a95e-7d85953d63c1" class="bulleted-list"><li style="list-style-type:circle"><strong>Video Streams</strong>: Analyzing and describing video sequences.</li></ul></li></ul><h2 id="e0f06f12-2ab7-40d8-9857-049f1872fb76" class="">Summary</h2><ul id="78c1aa77-e497-4a6d-b84d-4e8189b3f7d8" class="bulleted-list"><li style="list-style-type:disc"><strong>RNNs</strong>: Effective for tasks involving sequences and temporal data.</li></ul><ul id="971035a5-aee5-4011-b331-876f4795b9a8" class="bulleted-list"><li style="list-style-type:disc"><strong>LSTM Models</strong>: A specific type of RNN that handles long-term dependencies and is used in advanced applications like image and video analysis.</li></ul><hr id="1102d626-be73-49f2-bbed-641aed1d1144"/><h2 id="6e6aa85e-b653-42f8-9f14-8d8c5f70c64c" class="">Autoencoders and Restricted Boltzmann Machines (RBMs)</h2><h2 id="6396e49b-3826-408e-b813-cb423def81ed" class="">Introduction to Autoencoders</h2><ul id="bc9e1af6-2a79-4768-844c-13f7859bf137" class="bulleted-list"><li style="list-style-type:disc"><strong>Definition</strong>: Autoencoders are unsupervised deep learning models used for data compression. They learn to compress and decompress data automatically through neural networks.</li></ul><ul id="687872cf-c349-4135-b968-f8b8747154f5" class="bulleted-list"><li style="list-style-type:disc"><strong>Purpose</strong>: They aim to learn a compressed representation of the input data and then reconstruct the original data from this representation.</li></ul><ul id="30e37a9e-16eb-4212-92ae-60f9b7cc651d" class="bulleted-list"><li style="list-style-type:disc"><strong>Training</strong>: Autoencoders use backpropagation with the target variable being the same as the input, effectively learning an approximation of an identity function.</li></ul><h2 id="03fe3398-a8c0-49cc-8882-ab28263eae2c" class="">Architecture of an Autoencoder</h2><ul id="f7c1d8e6-48f5-4d26-842e-4d29afb8deac" class="bulleted-list"><li style="list-style-type:disc"><strong>Encoder</strong>:<ul id="a79477ad-e954-41fd-ab25-46b8b2440f5e" class="bulleted-list"><li style="list-style-type:circle">Compresses the input data into a lower-dimensional representation.</li></ul></li></ul><ul id="d6864492-2807-46b7-b152-2f55b16d3dd7" class="bulleted-list"><li style="list-style-type:disc"><strong>Decoder</strong>:<ul id="3e7ef496-9df9-4aaa-96d7-5ffb6bdf4a09" class="bulleted-list"><li style="list-style-type:circle">Reconstructs the original data from the compressed representation.</li></ul></li></ul><figure id="dcb559c3-b930-4c77-9476-252463a3dfd9" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%209.png"><img style="width:708px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%209.png"/></a></figure><h2 id="a0e02b8e-8050-490f-8b6e-a387c9ca06ce" class="">Applications of Autoencoders</h2><ul id="685a88ec-e332-451a-988c-ba0b3ce6841f" class="bulleted-list"><li style="list-style-type:disc"><strong>Data Denoising</strong>: Removing noise from data to recover the original signal.</li></ul><ul id="8bb18f01-a540-4d64-a7cc-574aba58c770" class="bulleted-list"><li style="list-style-type:disc"><strong>Dimensionality Reduction</strong>: Reducing the number of features in the data for visualization purposes.</li></ul><h2 id="beaf2b33-cfdf-474e-9b2e-fe3e4b4aa830" class="">Advantages over Traditional Methods</h2><ul id="6a3dd384-8b67-4121-bbd0-ea5136807328" class="bulleted-list"><li style="list-style-type:disc"><strong>Non-Linear Transformations</strong>: Autoencoders, with their non-linear activation functions, can learn more complex projections compared to linear methods like Principal Component Analysis (PCA).</li></ul><h2 id="dd950cc2-e026-49d2-8d61-49f452e61f61" class="">Restricted Boltzmann Machines (RBMs)</h2><ul id="6869ac16-2cdf-432b-822d-a6e2a40b4e1e" class="bulleted-list"><li style="list-style-type:disc"><strong>Overview</strong>: RBMs are a type of autoencoder that can learn the distribution of the input data to perform tasks such as:<ul id="310784f6-3637-44b7-b10a-ffba61459fc5" class="bulleted-list"><li style="list-style-type:circle"><strong>Fixing Imbalanced Datasets</strong>: Generating more data points for minority classes to balance datasets.</li></ul><ul id="3d102e9b-29a1-400e-8268-07beaf9a5fe0" class="bulleted-list"><li style="list-style-type:circle"><strong>Estimating Missing Values</strong>: Predicting missing feature values in datasets.</li></ul><ul id="c1342f3f-5ba9-4f4c-ba80-5b5f369c072e" class="bulleted-list"><li style="list-style-type:circle"><strong>Automatic Feature Extraction</strong>: Learning features from unstructured data.</li></ul></li></ul><figure id="9b74dc04-9587-4004-baab-cce9ab19775b" class="image"><a href="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%2010.png"><img style="width:799px" src="Module%204%20Deep%20Learning%20Models%20ccafb3f1ff2042b189751c787448de2d/image%2010.png"/></a></figure><h2 id="6cdefee4-cc2b-4e4b-b6d2-5c6bcfa946fe" class="">Summary</h2><ul id="f7f22fd7-86df-4761-84f0-6e52348504f1" class="bulleted-list"><li style="list-style-type:disc"><strong>Autoencoders</strong>: Useful for data compression, denoising, and dimensionality reduction.</li></ul><ul id="16c2a7db-298c-4e81-a9e7-816714d497d8" class="bulleted-list"><li style="list-style-type:disc"><strong>RBMs</strong>: Specialized autoencoders effective for handling imbalanced datasets, estimating missing values, and feature extraction.</li></ul><p id="87af0983-bdbb-4adb-894f-de75668ceb17" class="">This concludes the introduction to autoencoders and Restricted Boltzmann Machines (RBMs).</p><hr id="163aa7c6-2f7f-420c-8e96-23f52fb5cef6"/><p id="c2a0aefd-2709-49ab-8152-1e5ed7f7ea41" class="">
</p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>